import os
import requests
from openai import OpenAI
import anthropic
import google.generativeai as genai
import re
from dotenv import load_dotenv

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """
The following is a conversation about a shot.

Create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached, 
such as a shot being approved (finalled) by the creative lead.

Just generate short/consise notes from the given conversation, without any header or footer text such as subject line or follow up questions.

Following is the conversation:
{conversation}

"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20"
}

# === SUMMARIZATION FUNCTIONS ===

def summarize_openai(conversation, model, client):
    """
    Summarize using OpenAI. Caller must provide a valid OpenAI client instance.
    """
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content

def summarize_claude(conversation, model, client):
    """
    Summarize using Claude. Caller must provide a valid anthropic.Anthropic client instance.
    """
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

def summarize_ollama(conversation, model, client):
    """
    Summarize using Ollama. Caller must provide a requests.Session or similar client instance.
    """
    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False}
    )
    return response.json()["response"]

def summarize_gemini(conversation, model, client):
    """
    Summarize using Gemini. Caller must provide a valid google.generativeai.GenerativeModel instance as client.
    """
    full_prompt = f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"
    response = client.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        )
    )
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    candidate = response.candidates[0]
    if candidate.finish_reason == 2:
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:
        raise Exception("Response blocked for other reasons")
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    return candidate.content.parts[0].text

def create_llm_client(provider, api_key=None, model=None):
    """
    Convenience function to create a client instance for supported LLM providers.
    Args:
        provider (str): 'openai', 'claude', 'ollama', or 'gemini'
        api_key (str, optional): API key for the provider (if required)
        model (str, optional): Model name for Gemini (required for Gemini)
    Returns:
        Client instance for use with summarize_* functions
    """
    provider = provider.lower()
    if provider == "openai":
        if not api_key:
            raise ValueError("OpenAI requires an api_key.")
        return OpenAI(api_key=api_key)
    elif provider == "claude":
        if not api_key:
            raise ValueError("Anthropic Claude requires an api_key.")
        return anthropic.Anthropic(api_key=api_key)
    elif provider == "ollama":
        return requests.Session()
    elif provider == "gemini":
        if not api_key:
            raise ValueError("Gemini requires an api_key.")
        if not model:
            raise ValueError("Gemini requires a model name.")
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

if __name__ == "__main__":
    import argparse
    import sys
    load_dotenv()

    parser = argparse.ArgumentParser(description="Test LLM summarization for a given conversation text.")
    parser.add_argument("--provider", required=True, choices=["openai", "claude", "ollama", "gemini"], help="LLM provider to use")
    parser.add_argument("--model", help="Model name (optional, will use default if not provided)")
    parser.add_argument("--input", required=True, help="Path to text file containing the conversation")
    args = parser.parse_args()

    # Load conversation text
    try:
        with open(args.input, "r") as f:
            conversation = f.read()
    except Exception as e:
        print(f"Error reading input file: {e}")
        sys.exit(1)

    # Select model
    model = args.model or DEFAULT_MODELS.get(args.provider)
    if not model:
        print(f"No model specified and no default available for provider {args.provider}.")
        sys.exit(1)

    # Get API key from environment
    api_key_env_map = {
        "openai": "OPENAI_API_KEY",
        "claude": "CLAUDE_API_KEY",
        "gemini": "GEMINI_API_KEY"
    }
    api_key = None
    if args.provider in api_key_env_map:
        api_key = os.getenv(api_key_env_map[args.provider])
        if not api_key:
            print(f"API key for {args.provider} not found in .env file (expected variable: {api_key_env_map[args.provider]}).")
            sys.exit(1)

    # Create client
    try:
        client = create_llm_client(args.provider, api_key=api_key, model=model)
    except Exception as e:
        print(f"Error creating client: {e}")
        sys.exit(1)

    # Summarize
    try:
        if args.provider == "openai":
            summary = summarize_openai(conversation, model, client)
        elif args.provider == "claude":
            summary = summarize_claude(conversation, model, client)
        elif args.provider == "ollama":
            summary = summarize_ollama(conversation, model, client)
        elif args.provider == "gemini":
            summary = summarize_gemini(conversation, model, client)
        else:
            print(f"Unsupported provider: {args.provider}")
            sys.exit(1)
        #print("\n=== LLM Summary Output ===\n")
        print(summary)
    except Exception as e:
        print(f"Error during summarization: {e}")
        sys.exit(1)
