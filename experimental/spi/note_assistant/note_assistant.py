import argparse
import os
import pandas as pd
import requests
from tqdm import tqdm
from openai import OpenAI
import anthropic
import google.generativeai as genai
import json
import re
from dotenv import load_dotenv

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """\
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """\
The following is a conversation about few shots. Conversation about each shot is given in the following format:

<topic1>
<speaker_initial>: <text>
<speaker_initial>: <text>
...

<topic2>
<speaker_initial>: <text>
<speaker_initial>: <text>
...

and so on. Here are some examples:

<EXAMPLE_INPUT_START>
pln150/307214:
KJ:Here we go. First up, I just want to confirm about the depth on phone with the My assumption it's a Yrad on them and adding some spec.
BH:Yes, that's… how I understood it.

bdf058/300854:
KJ:All right, we can move on. And then this one,… Chris's note was about balancing Neya's hair. So, I wanted to know if it was a hue or luminance or what that was referring to.
BH:would just make it so that the main pieces have a little bit less of a color shift.
<EXAMPLE_INPUT_END>

Note that the topic/conversation data may not always be correct and the same conversation about a specific context can continue across multiple topics/conversations. Try to
group the beginining and end of the review of specific shot(s) based on verbal clues, such as "Let us look at the next one...", "OK, moving to next" etc. These clues may not
always be there, try your best to identify the beginning and end of shot review contexts as much as you can.

Once these review groups are identified, create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached, 
such as a shot being approved (finalled) by the creative lead.

Return your response in this format:

<topic1>|<abridged_conversation_text>

<topic2, topic3>|<abridged_conversation_text>

Here is a sample response:

<EXAMPLE_OUTPUT_START>
pln150/307214|BH: Confirms that this to be a y-grad on them with some spec
<EXAMPLE_OUTPUT_END>

Following is the conversation:
{conversation}

<...>"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20" # Or gemini-pro, gemini-1.5-pro-latest
}

# === SUMMARIZATION FUNCTIONS ===

def summarize_openai(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content

def summarize_claude(conversation, model):
    client = anthropic.Anthropic(api_key=os.getenv("CLAUDE_API_KEY"))
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

def summarize_ollama(conversation, model):
    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False}
    )
    return response.json()["response"]

def summarize_gemini(conversation, model):
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    
    # Initialize the model
    gemini_model = genai.GenerativeModel(model)
    
    # Combine system prompt and user prompt
    full_prompt = f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"
    
    # Generate response
    response = gemini_model.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        )
    )
    
    # Check if response was blocked or empty
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    
    candidate = response.candidates[0]
    
    # Check finish reason
    if candidate.finish_reason == 2:  # SAFETY
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:  # RECITATION
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:  # OTHER
        raise Exception("Response blocked for other reasons")
    
    # Check if content exists
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    
    return candidate.content.parts[0].text
    
# === PARSING FUNCTION ===

def extract_topic_summaries(response_text):
    """
    Extract individual topic summaries from the LLM response using the new pipe format:
    <topic>|<abridged_conversation_text>
    
    The abridged_conversation_text can span multiple lines, and we treat the entire topic 
    (including comma-separated topics) as a single key.
    
    Returns a dictionary with topic keys and summary values
    """
    topic_summaries = {}
    # Split the response text into individual lines.
    # Each line is expected to be a 'topic|summary' pair.
    lines = response_text.strip().split('\n')

    # Regex to parse each line into topic and summary.
    # - Group 1 (topic): `([^|]+?)` captures one or more characters before the first pipe, non-greedily.
    #                    Ensures there's content before the pipe.
    # - Group 2 (summary): `(.*)` captures everything after the first pipe to the end of the line.
    # `^` and `$` anchor the match to the start and end of the line string.
    line_parser_re = re.compile(r"^\s*([^|]+?)\s*\|\s*(.*)$")

    for line_text in lines:
        line_text = line_text.strip() # Strip whitespace from the individual line
        if not line_text:  # Skip empty lines that might result from splitting
            continue
            
        match = line_parser_re.match(line_text)
        if match:
            topic = match.group(1).strip()
            summary = match.group(2).strip() # Summary is the content after pipe on this line
            
            if topic:  # Ensure the topic is not empty after stripping
                topic_summaries[topic] = summary
        # else:
            # Optional: Log or handle lines that don't match the "topic|summary" format.
            # print(f"Warning: Could not parse line: '{line_text}'")
    
    return topic_summaries


def load_initial_data(input_csv, pre_process, provider):
    """Loads the input CSV, prepares the initial result DataFrame, and initializes the LLM client."""
    # Read the input CSV
    df = pd.read_csv(input_csv)

    # Prepare output dataframe with results
    if pre_process:
        result_df = pd.DataFrame(columns=["chunk_id", "chunk_size", "shots_included", "chunk_content"])
    else:
        result_df = pd.DataFrame(columns=["chunk_id", "shots_included", "shot/id", "summary"])

    client = None
    if provider == "openai" and not pre_process:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # Gemini client is typically initialized within the summarize_gemini function or globally via genai.configure
    # No specific client object needed here for Gemini if using the global configuration approach.
    
    return df, result_df, client


def prepare_shot_list(df):
    """Processes the DataFrame into a list of shots, maintaining order and formatting content."""
    shot_id_to_indices = {}
    for idx, row in df.iterrows():
        shot_id_val = row["shot/id"]
        if shot_id_val not in shot_id_to_indices:
            shot_id_to_indices[shot_id_val] = []
        shot_id_to_indices[shot_id_val].append(idx)

    # Group by shot/id to keep individual shot conversations together
    grouped = df.groupby("shot/id")["conversation"].apply(lambda x: "\n".join(x)).reset_index()

    # Sort the grouped data by first occurrence in the original data
    grouped["first_idx"] = grouped["shot/id"].apply(lambda x: min(shot_id_to_indices[x]))
    grouped = grouped.sort_values("first_idx").drop("first_idx", axis=1)
    
    # Prepare shot data with formatted conversations and sizes
    shots = []
    for _, row in grouped.iterrows():
        shot_id_val = row["shot/id"]
        conversation = row["conversation"]
        formatted_conversation = f"{shot_id_val}:\n{conversation}\n\n"
        shots.append({
            "shot_id": shot_id_val,
            "content": formatted_conversation,
            "size": len(formatted_conversation),
            "original_indices": shot_id_to_indices[shot_id_val]
        })
    return shots

def build_content_chunks(shots, max_chars, overlap_chars):
    """Creates overlapping chunks from the list of shots based on character limits."""
    # Prepare to create chunks based on character count with overlap
    chunks = []
    current_chunk_shots = []
    current_chunk_size = 0
    overlap_buffer = []
    overlap_buffer_size = 0
    
    # Process each shot to form chunks
    for shot in shots:
        shot_size = shot["size"]
        
        # If this single shot exceeds the max size, include it alone
        # but warn about it exceeding the limit
        if shot_size > max_chars:
            print(f"WARNING: Shot {shot['shot_id']} has size {shot_size} chars, which exceeds the max_chars limit of {max_chars}.")
            chunks.append({
                "content": shot["content"],
                "size": shot_size,
                "shots": [shot["shot_id"]],
                "indices": shot["original_indices"]
            })
            
            # This shot becomes the overlap buffer for the next chunk
            if overlap_chars > 0:
                overlap_buffer = [shot]
                overlap_buffer_size = shot_size
            else:
                overlap_buffer = []
                overlap_buffer_size = 0
            
            # Reset current chunk
            current_chunk_shots = []
            current_chunk_size = 0
            continue
            
        # If adding this shot would exceed the limit, finalize current chunk and start a new one
        if current_chunk_size + shot_size > max_chars and current_chunk_shots:
            # Finalize current chunk
            chunk_content = "".join([s["content"] for s in current_chunk_shots])
            chunk_shots = [s["shot_id"] for s in current_chunk_shots]
            all_indices = [idx for s in current_chunk_shots for idx in s["original_indices"]]
            chunks.append({
                "content": chunk_content,
                "size": current_chunk_size,
                "shots": chunk_shots,
                "indices": all_indices
            })
            
            # Start new chunk with overlap if enabled
            if overlap_chars > 0:
                # Find shots to include in overlap
                overlap_buffer = []
                overlap_buffer_size = 0
                
                # Add shots from the end of the previous chunk to the overlap buffer until we reach overlap_chars
                # or run out of shots
                for s in reversed(current_chunk_shots):
                    if overlap_buffer_size + s["size"] <= overlap_chars:
                        overlap_buffer.insert(0, s)  # Insert at beginning to maintain order
                        overlap_buffer_size += s["size"]
                    else:
                        # If this shot would exceed overlap_chars, we need to decide if we include it
                        # We'll include it if we haven't added any shots yet or if including it gets us closer to target
                        if not overlap_buffer or (overlap_chars - overlap_buffer_size) > (overlap_buffer_size + s["size"] - overlap_chars):
                            overlap_buffer.insert(0, s)
                            overlap_buffer_size += s["size"]
                        break
                
                # Initialize new chunk with overlap buffer
                current_chunk_shots = overlap_buffer.copy()
                current_chunk_size = overlap_buffer_size
            else:
                # No overlap
                current_chunk_shots = []
                current_chunk_size = 0
        
        # Add this shot to the current chunk
        current_chunk_shots.append(shot)
        current_chunk_size += shot_size
    
    # Add the last chunk if it has content
    if current_chunk_shots:
        chunk_content = "".join([s["content"] for s in current_chunk_shots])
        chunk_shots = [s["shot_id"] for s in current_chunk_shots]
        all_indices = [idx for s in current_chunk_shots for idx in s["original_indices"]]
        chunks.append({
            "content": chunk_content,
            "size": current_chunk_size,
            "shots": chunk_shots,
            "indices": all_indices
        })

    # Sort chunks based on the earliest index in each chunk to maintain original order
    chunks.sort(key=lambda x: min(x["indices"]) if x["indices"] else float('inf'))
    return chunks
def process_content_chunks(chunks, provider, model, client, pre_process, initial_result_df, output_llm_response_csv=None, input_llm_response_path=None, verbose=False):
    """Processes each content chunk, either by LLM summarization or for pre-processing."""
    result_df = initial_result_df.copy() # Work on a copy
    llm_responses_data = []
    cached_llm_responses = None

    desc_text = "Processing chunks"
    if pre_process:
        desc_text += " (pre-processing)"
    elif input_llm_response_path:
        try:
            cached_df = pd.read_csv(input_llm_response_path)
            if 'chunk_id' not in cached_df.columns:
                print(f"⚠️ Warning: Cached LLM response CSV {input_llm_response_path} must contain a 'chunk_id' column. Proceeding without cache.")
                input_llm_response_path = None # Disable cache use
            else:
                cached_llm_responses = cached_df.set_index('chunk_id')
                desc_text += f" from cache ({input_llm_response_path})"
                if verbose:
                    print(f"ℹ️ Loaded {len(cached_llm_responses)} cached LLM responses from {input_llm_response_path}")
        except Exception as e:
            print(f"⚠️ Warning: Could not load or process cached LLM responses from {input_llm_response_path}: {e}. Proceeding without cache.")
            input_llm_response_path = None # Disable cache use
            desc_text += f" with {provider}" if provider and provider != "none" else ""
    elif provider and provider != "none":
        desc_text += f" with {provider}"

    for i, chunk in enumerate(tqdm(chunks, desc=desc_text)):
        chunk_id = i + 1
        conversation_text = chunk["content"]
        shots_included = ", ".join(chunk["shots"])
        
        if pre_process:
            # Just output the chunked conversations without LLM processing
            new_row_content = {
                "chunk_id": [chunk_id],
                "chunk_size": [chunk["size"]],
                "shots_included": [shots_included],
                "chunk_content": [conversation_text]
            }
            result_df = pd.concat([result_df, pd.DataFrame(new_row_content)], ignore_index=True)
            continue
        
        raw_response_for_log = None
        error_for_log = None
        response_source = "live_llm" # Default source
        cached_data_row = None

        try:
            if input_llm_response_path and cached_llm_responses is not None:
                try:
                    cached_data_row = cached_llm_responses.loc[chunk_id]
                    response = cached_data_row['raw_llm_response']
                    # If the cached response itself was an error, it will be processed by extract_topic_summaries
                    # or caught if it's a malformed error string.
                    raw_response_for_log = response
                    response_source = "cache"
                    # Check if the cached entry had an error
                    if pd.notna(cached_data_row.get('error_message')):
                        error_for_log = cached_data_row.get('error_message')

                except KeyError:
                    error_msg = f"Chunk ID {chunk_id} not found in cached LLM responses from {input_llm_response_path}. Skipping LLM processing for this chunk."
                    print(f"⚠️ Warning: {error_msg}")
                    raw_response_for_log = f"Error: {error_msg}"
                    error_for_log = error_msg
                    response_source = "cache_miss"
                    result_df = pd.concat([
                        result_df,
                        pd.DataFrame({
                            "chunk_id": [chunk_id],
                            "shots_included": [shots_included],
                            "shot/id": ["CACHE_MISS_ERROR"],
                            "summary": [raw_response_for_log]
                        })
                    ], ignore_index=True)
                    # Log this attempt if output_llm_response_csv is active
                    if output_llm_response_csv: # No 'not pre_process' check needed here as we continue
                        llm_responses_data.append({
                            "chunk_id": chunk_id, "provider": provider, "model": model,
                            "input_conversation_text": conversation_text,
                            "raw_llm_response": raw_response_for_log,
                            "error_message": error_for_log, "source": response_source
                        })
                    continue # Move to the next chunk
            else: # Perform live LLM call
                if provider == "openai":
                    response = summarize_openai(conversation_text, model, client)
                elif provider == "claude":
                    response = summarize_claude(conversation_text, model)
                elif provider == "ollama":
                    response = summarize_ollama(conversation_text, model)
                elif provider == "gemini":
                    response = summarize_gemini(conversation_text, model)
                else:
                    raise ValueError(f"Unsupported or no provider specified for live LLM call: {provider}")
                raw_response_for_log = response # Set for logging
                
            # Extract topic summaries from the response
            topic_summaries = extract_topic_summaries(response)

            # When using pre-processed mode, we want to maintain the exact original format
            # including composite topics (e.g., "topic1, topic2")
            for topic, summary in topic_summaries.items():
                result_df = pd.concat([
                    result_df,
                    pd.DataFrame({
                        "chunk_id": [chunk_id],
                        "shots_included": [shots_included],
                        "shot/id": [topic],  # Keep composite topics as a single entity
                        "summary": [summary]  # This can be multi-line text
                    })
                ], ignore_index=True)
                
        except Exception as e: # Catches errors from LLM call OR from extract_topic_summaries OR bad cache entry
            # Ensure raw_response_for_log is set if it's an early error (e.g. provider ValueError)
            if raw_response_for_log is None:
                raw_response_for_log = f"Error: {str(e)}"
            error_for_log = str(e)
            print(f"Error processing chunk {chunk_id}: {str(e)}")
            result_df = pd.concat([
                result_df,
                pd.DataFrame({
                    "chunk_id": [chunk_id],
                    "shots_included": [shots_included],
                    "shot/id": ["ERROR"],
                    "summary": [f"Processing error: {str(e)}"]
                })
            ], ignore_index=True)

        # Log if output_llm_response_csv is enabled (and not pre_processing, though covered by continue)
        if output_llm_response_csv:
            log_provider = provider
            log_model = model
            if response_source == "cache" and cached_data_row is not None:
                log_provider = cached_data_row.get('provider', provider) # Prefer cached, fallback to CLI
                log_model = cached_data_row.get('model', model)         # Prefer cached, fallback to CLI

            llm_responses_data.append({
                "chunk_id": chunk_id,
                "provider": log_provider,
                "model": log_model,
                "input_conversation_text": conversation_text,
                "raw_llm_response": raw_response_for_log,
                "error_message": error_for_log,
                "source": response_source
            })

    if output_llm_response_csv and llm_responses_data:
        llm_df = pd.DataFrame(llm_responses_data)
        llm_df.to_csv(output_llm_response_csv, index=False)
        if verbose:
            print(f"📝 LLM request/response data saved to {output_llm_response_csv}")
    return result_df

def get_prod_notes_for_row(row_shot_id_field, prod_notes_map):
    """Helper function to retrieve and format prod notes for a given shot/id field."""
    if pd.isna(row_shot_id_field) or not prod_notes_map:
        return ""
    individual_ids = [s_id.strip() for s_id in str(row_shot_id_field).split(',')]
    notes_found = []
    for s_id in individual_ids:
        if s_id in prod_notes_map and pd.notna(prod_notes_map[s_id]):
            notes_found.append(str(prod_notes_map[s_id]))
    return "\n---\n".join(notes_found) # Join multiple notes with a separator

def save_output(result_df, output_csv, num_chunks, prod_notes_map=None, chunk_content_map=None, verbose=False):
    """Saves the processed DataFrame to a CSV file and prints a confirmation, including overlap metadata."""
    
    # Add "prod notes" column if a map is provided and 'shot/id' column exists (i.e., not in pre-process mode)
    if prod_notes_map is not None and "shot/id" in result_df.columns:
        result_df["prod notes"] = result_df["shot/id"].apply(lambda x: get_prod_notes_for_row(x, prod_notes_map))

    # Add "original_conversation" column if a map is provided and not in pre-process mode
    if chunk_content_map is not None and "chunk_id" in result_df.columns:
        # Ensure 'chunk_id' is integer if it's not already, for reliable mapping
        # This is important if chunk_id was read from a cache CSV and might be float/object
        result_df["original_conversation"] = result_df["chunk_id"].astype(int).apply(lambda cid: chunk_content_map.get(cid, ""))

    # Save results to output CSV
    result_df.to_csv(output_csv, index=False)
    # Removed overlap_chars from the print message as the column is removed.
    # If you still want to log it, you'd need to pass overlap_chars to this function.
    if verbose:
        print(f"✅ Output saved to {output_csv} with {num_chunks} chunks processed.")
# === MAIN DRIVER FUNCTION ===

def main(input_csv, output_csv, provider, model, max_chars=8000, overlap_chars=0, pre_process=False, output_llm_response_csv=None, input_llm_response_path=None, review_filter_ids=None, prod_notes_map=None, verbose=False):
    # 1. Load and prepare initial data and client
    df, result_df, client = load_initial_data(input_csv, pre_process, provider)

    # 2. Prepare the list of shots with their content and sizes
    # This function groups by 'shot/id' and formats the content
    if verbose:
        print("Preparing shot list from input CSV...")
    shots = prepare_shot_list(df)

    # 3. Build content chunks based on character limits and overlap
    chunks = build_content_chunks(shots, max_chars, overlap_chars)

    # Create a map of chunk_id to its content for later use in save_output
    chunk_id_to_content_map = {i + 1: chunk["content"] for i, chunk in enumerate(chunks)}

    # 4. Process these chunks (either LLM summarization or pre-processing)
    
    # Filter chunks based on review_filter_ids if provided
    if review_filter_ids:
        original_chunk_count = len(chunks)
        chunks = [chunk for chunk in chunks if any(shot_id in review_filter_ids for shot_id in chunk["shots"])]
        if verbose:
            print(f"Filtered chunks: Kept {len(chunks)} out of {original_chunk_count} based on --review filter.")
        if not chunks:
            print("No chunks match the specified --review filter. Exiting.")
            return # Exit if no chunks remain after filtering

    final_result_df = process_content_chunks(chunks, provider, model, client, pre_process, result_df, output_llm_response_csv, input_llm_response_path, verbose=verbose)

    # 5. Save the results
    save_output(final_result_df, output_csv, len(chunks), prod_notes_map, chunk_id_to_content_map if not pre_process else None, verbose=verbose)

# === CLI ARGUMENT PARSING ===

if __name__ == "__main__":
    # Load environment variables from .env file if it exists
    load_dotenv()

    parser = argparse.ArgumentParser(
        description="Processes meeting transcripts from a CSV file to generate concise summaries or abridged conversations for each topic (e.g., movie shots) using Large Language Models (LLMs). "
                    "Features include content chunking with configurable overlap for handling large transcripts, support for multiple LLM providers (OpenAI, Claude, Ollama, Gemini), "
                    "caching of LLM responses, and a pre-processing mode to inspect chunked data before LLM submission.")
    parser.add_argument("input_csv",
                        help="Path to the input CSV file containing meeting transcripts. Expected columns: 'shot/id' (topic identifier) and 'conversation' (dialogue text).")
    parser.add_argument("output_csv",
                        help="Path to the output CSV file where results will be saved. "
                             "In LLM mode, contains 'shot/id' and 'summary'. "
                             "In --pre-process mode, contains 'chunk_id', 'chunk_size', 'shots_included', and 'chunk_content'.")
    parser.add_argument("--provider", required=False, choices=["openai", "claude", "ollama", "gemini"],
                        help="LLM provider to use")
    parser.add_argument("--model", help="Optional model override (e.g., gpt-3.5-turbo, claude-3-opus-20240229, llama3)")
    parser.add_argument("--max-chars", type=int, default=8000,
                        help="Maximum number of characters to include in a single chunk")
    parser.add_argument("--overlap-chars", type=int, default=1000,
                        help="Number of characters to overlap between consecutive chunks")
    parser.add_argument("--pre-process", action="store_true",
                        help="Skip LLM processing and output grouped conversations to CSV")
    parser.add_argument("--output-llm-response",
                        help="Optional path to CSV file to store LLM request/response pairs")
    parser.add_argument("--input-llm-response",
                        help="Optional path to a CSV file with cached LLM responses to re-process instead of live calls.")
    parser.add_argument("--review", help="Comma-separated list of 'shot/id' values to filter processing. Only chunks containing these IDs will be processed.")
    parser.add_argument("--review-csv", help="Path to a CSV file with 'shot/id' and 'notes' columns. 'shot/id's are used for filtering, 'notes' are added to output as 'prod notes'.")
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose output for more detailed processing information.")


    args = parser.parse_args()

    # API key check - only if not in pre-process mode
    if not args.pre_process:
        if not args.input_llm_response: # Only require provider if not using cached input and not pre-processing
            if not args.provider:
                raise ValueError("--provider is required unless --pre-process or --input-llm-response is specified.")
            
            if args.provider == "openai":
                if not os.getenv("OPENAI_API_KEY"):
                    raise EnvironmentError("Set OPENAI_API_KEY in environment for live OpenAI calls.")
            elif args.provider == "claude":
                if not os.getenv("CLAUDE_API_KEY"):
                    raise EnvironmentError("Set CLAUDE_API_KEY in environment for live Claude calls.")
            elif args.provider == "gemini":
                if not os.getenv("GOOGLE_API_KEY"):
                    raise EnvironmentError("Set GOOGLE_API_KEY in environment for live Gemini calls.")

            selected_model = args.model or DEFAULT_MODELS.get(args.provider)
            if not selected_model: # Should not happen if provider is in DEFAULT_MODELS
                raise ValueError(f"No default model available for provider {args.provider}. Please specify with --model.")
        else: # Using input_llm_response, provider/model from CLI are secondary
            selected_model = args.model or DEFAULT_MODELS.get(args.provider, "cache_default") # Provide a fallback model name
            print(f"ℹ️ Using cached LLM responses from: {args.input_llm_response}. CLI provider/model will be used for logging or if cache is incomplete.")
    else:
        # Placeholder values for pre-process mode
        args.provider = "none"
        selected_model = "none"
        
    if args.max_chars <= 0:
        raise ValueError("max-chars must be a positive number")
    
    if args.overlap_chars < 0:
        raise ValueError("overlap-chars must be a non-negative number")
    
    if args.overlap_chars >= args.max_chars:
        raise ValueError("overlap-chars must be less than max-chars")
        
    # Process the --review and --review-csv arguments
    review_filter_ids_list = []
    prod_notes_map = None # Initialize to None, will be a dict if --review-csv is used

    if args.review:
        review_filter_ids_list.extend([item.strip() for item in args.review.split(',') if item.strip()])

    if args.review_csv:
        prod_notes_map = {} # Will store shot/id -> notes
        try:
            review_df = pd.read_csv(args.review_csv)
            if "shot/id" not in review_df.columns or "notes" not in review_df.columns:
                print(f"⚠️ Warning: --review-csv file '{args.review_csv}' must contain 'shot/id' and 'notes' columns. Skipping its use for filtering and notes.")
            else:
                if args.verbose:
                    print(f"ℹ️ Processing --review-csv file: {args.review_csv}")
                # Add shot/ids from CSV to the filter list
                review_df_shot_ids = review_df["shot/id"].dropna().astype(str).tolist()
                review_filter_ids_list.extend(review_df_shot_ids)
                # Populate the prod_notes_map
                for _, row in review_df.iterrows(): # No tqdm here, usually small file
                    shot_id = str(row["shot/id"])
                    notes = str(row["notes"]) if pd.notna(row["notes"]) else ""
                    if shot_id: # Ensure shot_id is not empty
                        prod_notes_map[shot_id] = notes
        except FileNotFoundError:
            print(f"⚠️ Warning: --review-csv file not found: '{args.review_csv}'. Skipping its use.")
        except Exception as e:
            print(f"⚠️ Warning: Error processing --review-csv file '{args.review_csv}': {e}. Skipping its use.")
    
    final_review_filter_ids = list(set(review_filter_ids_list)) if review_filter_ids_list else None

    main(args.input_csv, args.output_csv, args.provider, selected_model, 
         max_chars=args.max_chars, overlap_chars=args.overlap_chars, pre_process=args.pre_process,
         output_llm_response_csv=args.output_llm_response,
         input_llm_response_path=args.input_llm_response,
         review_filter_ids=final_review_filter_ids,
         prod_notes_map=prod_notes_map,
         verbose=args.verbose)